{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8484c37f",
   "metadata": {},
   "source": [
    "# class transformers.PegasusForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e409ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91838\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:871: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  obj = cast(Storage, torch._UntypedStorage(nbytes))\n",
      "Some weights of the model checkpoint at google/pegasus-large were not used when initializing PegasusForCausalLM: ['model.encoder.layers.13.fc2.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.13.self_attn_layer_norm.bias', 'model.encoder.layers.14.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.12.fc2.weight', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.13.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.encoder.layers.15.fc1.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.encoder.layers.14.fc2.weight', 'model.encoder.layers.15.self_attn_layer_norm.weight', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.14.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.12.final_layer_norm.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.14.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.13.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.weight', 'model.encoder.layers.15.fc2.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.14.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.14.fc1.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.13.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.12.fc1.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.12.self_attn.v_proj.weight', 'model.encoder.layers.13.fc1.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.12.fc2.bias', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.12.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.13.self_attn.out_proj.weight', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'final_logits_bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.13.fc1.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.15.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layer_norm.bias', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.15.self_attn_layer_norm.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.12.final_layer_norm.bias', 'model.encoder.layers.14.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.14.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.13.fc2.weight', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.14.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.13.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.15.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.q_proj.weight', 'model.encoder.layers.13.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.14.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.15.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.13.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.12.self_attn_layer_norm.weight', 'model.shared.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.12.fc1.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.15.final_layer_norm.weight', 'model.encoder.layers.12.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layer_norm.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.12.self_attn.q_proj.weight', 'model.encoder.layers.14.fc2.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.15.fc1.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.12.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing PegasusForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForCausalLM were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['lm_head.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForCausalLM\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForCausalLM.from_pretrained(\"google/pegasus-large\", add_cross_attention=False)\n",
    "assert model.config.is_decoder, f\"{model.__class__} has to be configured as a decoder.\"\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "expected_shape = [1, inputs.input_ids.shape[-1], model.config.vocab_size]\n",
    "list(logits.shape) == expected_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140420ed",
   "metadata": {},
   "source": [
    "forward\n",
    "\n",
    "( input_ids: LongTensor = Noneattention_mask: typing.Optional[torch.Tensor] = Noneencoder_hidden_states: typing.Optional[torch.FloatTensor] = Noneencoder_attention_mask: typing.Optional[torch.FloatTensor] = Nonehead_mask: typing.Optional[torch.Tensor] = Nonecross_attn_head_mask: typing.Optional[torch.Tensor] = Nonepast_key_values: typing.Optional[typing.List[torch.FloatTensor]] = Noneinputs_embeds: typing.Optional[torch.FloatTensor] = Nonelabels: typing.Optional[torch.LongTensor] = Noneuse_cache: typing.Optional[bool] = Noneoutput_attentions: typing.Optional[bool] = Noneoutput_hidden_states: typing.Optional[bool] = Nonereturn_dict: typing.Optional[bool] = None ) → transformers.modeling_outputs.CausalLMOutputWithCrossAttentions or tuple(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6d75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
