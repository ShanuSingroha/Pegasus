{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514dc0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467edf3e8d024f8492349db4ab3aa258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "978b33f1080a4c8fb27ec5f9e808846e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af4e954a5734fb6830265ac13b4d0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8339aa3344a143b7880b88132d376863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c283dda476547fbb8e314dc577e38be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91838\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:871: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  obj = cast(Storage, torch._UntypedStorage(nbytes))\n",
      "Some weights of the model checkpoint at google/pegasus-large were not used when initializing PegasusModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing PegasusModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PegasusModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PegasusModel were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 1024]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusModel\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusModel.from_pretrained(\"google/pegasus-large\")\n",
    "\n",
    "inputs = tokenizer(\"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\")\n",
    "decoder_inputs = tokenizer(\"Studies show that\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids=inputs.input_ids, decoder_input_ids=decoder_inputs.input_ids)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9676933",
   "metadata": {},
   "source": [
    "Parameters\n",
    "\n",
    "config (PegasusConfig) — Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the from_pretrained() method to load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a59836",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward\n",
    "\n",
    "( input_ids: typing.Optional[torch.Tensor] = Noneattention_mask: typing.Optional[torch.Tensor] = \n",
    " Nonedecoder_input_ids: typing.Optional[torch.Tensor] = Nonedecoder_attention_mask: typing.Optional[torch.Tensor] \n",
    " = Nonehead_mask: typing.Optional[torch.Tensor] = Nonedecoder_head_mask: typing.Optional[torch.Tensor] =\n",
    " Nonecross_attn_head_mask: typing.Optional[torch.Tensor] = Noneencoder_outputs: typing.Optional[typing.Tuple[torch.FloatTensor]]\n",
    " = Nonepast_key_values: typing.Optional[typing.Tuple[torch.FloatTensor]] = Noneinputs_embeds: typing.Optional[torch.Tensor]\n",
    " = Nonedecoder_inputs_embeds: typing.Optional[torch.Tensor] = Noneuse_cache: typing.Optional[bool] = \n",
    " Noneoutput_attentions: typing.Optional[bool] = Noneoutput_hidden_states: typing.Optional[bool]\n",
    " = Nonereturn_dict: typing.Optional[bool] = None ) → transformers.modeling_outputs.Seq2SeqModelOutput or \n",
    "tuple(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e459cc",
   "metadata": {},
   "source": [
    "input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide it.\n",
    "Indices can be obtained using PegasusTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdce7e",
   "metadata": {},
   "source": [
    "attention_mask (torch.Tensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:\n",
    "1 for tokens that are not masked,\n",
    "0 for tokens that are masked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddaa92",
   "metadata": {},
   "source": [
    "decoder_input_ids (torch.LongTensor of shape (batch_size, target_sequence_length), optional) — Indices of decoder input sequence tokens in the vocabulary.\n",
    "Indices can be obtained using PegasusTokenizer. See PreTrainedTokenizer.encode() and PreTrainedTokenizer.call() for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323322f",
   "metadata": {},
   "source": [
    "decoder_attention_mask (torch.LongTensor of shape (batch_size, target_sequence_length), optional) — Default behavior: generate a tensor that ignores pad tokens in decoder_input_ids. Causal mask will also be used by default.\n",
    "\n",
    "\n",
    "head_mask (torch.Tensor of shape (encoder_layers, encoder_attention_heads), optional) — Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in [0, 1]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a79b1d",
   "metadata": {},
   "source": [
    "decoder_head_mask (torch.Tensor of shape (decoder_layers, decoder_attention_heads), optional) — Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in [0, 1]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb73c96",
   "metadata": {},
   "source": [
    "encoder_outputs (tuple(tuple(torch.FloatTensor), optional) — Tuple consists of (last_hidden_state, optional: hidden_states, optional: attentions) last_hidden_state of shape (batch_size, sequence_length, hidden_size), optional) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
    "\n",
    "\n",
    "past_key_values (tuple(tuple(torch.FloatTensor)), optional, returned when use_cache=True is passed or when config.use_cache=True) — Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2 tensors of shape (batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape (batch_size, num_heads, encoder_sequence_length, embed_size_per_head)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041efcd4",
   "metadata": {},
   "source": [
    "use_cache (bool, optional) — If set to True, past_key_values key value states are returned and can be used to speed up decoding (see past_key_values).\n",
    "\n",
    "\n",
    "output_attentions (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See attentions under returned tensors for more detail.\n",
    "\n",
    "\n",
    "output_hidden_states (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more detail.\n",
    "\n",
    "\n",
    "return_dict (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be193e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
